# 3 线性模型
## 3.1 基本形式
$$ f(\mathbf{x})=w_1x_1+w_2x_2+\cdots+w_dx_d+b $$
向量形式：
$$ f(\mathbf{x})=\mathbf{w}^T\mathbf{x}+b $$
## 3.2 线性回归
- 均方误差（只考虑一个属性值）：
$$ E(w,b)=\sum_{i=1}^m (y_i-wx_i-b)^2 $$
有：
$$ \frac{\partial E}{\partial w}=2(w\sum{x_i^2}-\sum(y_i-b)x_i) $$
$$ \frac{\partial E}{\partial b}=2(mb-\sum{(y_i-wx_i})) $$
- 多元线性回归：
$$ \hat{\mathbf{w}}=(\mathbf{w};b) $$
- 若解出多个w，需要由算法偏好决定，如引入正则化项。

- 模型预测值逼近y的衍生物：
$$ \ln y=\mathbf{w}^T\mathbf{x}+b $$
## 3.3 对数几率回归
- 对数几率函数 logistic function
$$ y=\frac{1}{1+e^{-z}} $$
- 对数几率
y 视为样本 x 为正例的可能性，y与1-y的比值为几率，因此对数几率为
$$ \ln{\frac{y}{1-y}} $$

- 极大似然优化
最小化：
$$ l(\beta)=\sum_{i=1}^{m}(-y_i\beta^T\hat{x_i}+\ln(1+e^{\beta^T\hat{x_i}})) $$
使用数值优化算法（梯度下降法，牛顿法）
这里 $y_i$ 只能为0或1

## 3.4 线性判别分析
- Linear Discriminant Analysis
投影到直线，同近异远

- 基本的 解析几何？
均值向量，协方差矩阵：
$$ \mathbf{\mu_i, \Sigma_i} $$
直线：
$$ \mathbf{\omega} $$
则中心，协方差：
$$ \mathbf{\omega^T\mu_i, \omega^T\Sigma_i\omega} $$
- 目标：
同类点协方差尽可能小，异类点中心距离尽可能大。
从而可得到最大化目标 p77
解得 w p78

## 3.5 多分类学习
- 拆分策略
一对一 OVO：训练$\frac{N(N-1)}{2}$个分类器
一对其余 OVR：训练$N$个分类器
多对多 MVM

- 一种MVM技术：纠错输出码
编码（划分）——解码（比较）

## 3.6 类别不平衡问题
- 基本策略：再放缩
欠采样、过采样、阈值迁移

