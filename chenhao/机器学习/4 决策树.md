# 4 决策树
## 4.1 基本流程
算法 p90
重点是如何选择最优划分属性
## 4.2划分选择
- 信息熵
度量样本“纯度”
$$ Ent(D)=-\sum{p_k\log_2 p_k} $$
- 信息增益
进行一次分支后各节点信息熵下降的加权平均值
$$ Gain(D,a)=Ent(a)-\sum{\frac{|D^v|}{|D|}Ent(D^v)} $$
但是对可取值较多的属性有偏好

- 增益率
$$ Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)} $$
$$ IV(a)=-\sum\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|} $$
但是对可选值较少的属性有偏好

- 基尼值
另一种度量样本纯度的方式
$$ Gini(D)=1-\sum p_k^2 $$
即随机抽取两个样本，其类别不一致的概率
类比信息增益，可以定义基尼指数$Gini\_index(D,a)$

## 4.3 剪枝处理
主动去掉一些分支来降低过拟合的风险

- 预剪枝
留出法做验证集，然后每次判断所选择的属性能否提高在验证集上的准确率。
具体过程 p97

- 后剪枝
先生成完整的决策树，再依次查看如果对各非叶子节点进行剪枝后精度能否得到提高。
具体过程 p98

## 4.4 连续与缺失值

- 连续值处理
对样本中连续属性a的n个取值，可以取n-1个划分点，然后选取划分后信息增益$Gain(D,a)$最大的点作为划分点。
各划分点取值为相邻属性值的算数平均值。

- 与离散属性不同，连续属性还可最为其后代的划分属性。

### 4.4.2 缺失值处理
- 两个问题：
1. 如何选择属性划分
2. 给定属性划分，若样本属性值缺失应该怎么处理

- 选取最优划分属性
对属性a，选取未缺失属性值的样本子集$D'$来计算信息增益，然后乘上$\rho=\frac{|D'|}{|D|}$作为属性a的优劣选择。
每个样本有个样本权值，划分后各节点计算信息增益所用到的权值是节点内样本与总围墙上属性样本依据样本权值的占比。

- 处理样本缺失属性
对于属性值缺失的样本，则将其同时分入各子节点，样本权值乘上对应子节点的权值。
具体过程 p104

## 4.5 多变量决策树
- 混合属性划分
非叶节点不再是对单个属性，而是对属性的线性组合进行测试。
每个节点是形如$\sum_{i=1}^dw_ia_i=t$的线性分类器
