# 5 神经网络
## 5.1 神经元模型
$$ y_j=f(\sum w_ix_i-\theta_j) $$

## 5.2感知机与多层网络
- 单层网络只能解决线性可分问题

## 5.3 误差逆传播算法
 
- 梯度下降
$$ \Delta w=-\eta\frac{\partial E_k}{\partial w} $$
- 缓解过拟合
早停，正则化。

## 5.4 全局最小和局部最小
- 跳出局部最小的方法
1. 用多组参数初始值训练网络
2. 模拟退火
3. 随机梯度下降
4. 遗传算法 （不知道是什么东西？）
上述技术多为启发式，理论上缺乏保障

## 5.5 其他常见神经网络
### 5.5.1 RBF网络
Radial Basis Function

- 单隐层前馈神经网络
$$ \phi(x)=\sum w_i\rho(\mathbf x_i,\mathbf c_i) $$
其中$\rho(\mathbf x_i,\mathbf c_i)$是径向基函数 p124
神经元中心$\mathbf c_i$是超参数

- 具有足够多的隐层神经元的RBF网络能以任意精度逼近任意连续函数

### 5.5.2 ART网络
具体过程 p125

- 竞争型学习
每一时刻有且仅有一个神经元被激活。

- 可塑性-稳定性窘境

### 5.5.3 SOM网络
将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构。

### 5.5.4 级联相关网络
- 结构自适应网络将网络结构也当作学习目标之一。

- 级联：建立层次连接的层级网络
- 相关：最大化输出与网络误差的相关性来训练参数

### 5.5.5 Elman网络
- 递归神经网络
将一些神经元的输出反馈回来作为输入信号

### 5.5.6 Boltzmann机

## 5.6 深度学习
- 节省训练开销：无监督逐层训练、权共享
